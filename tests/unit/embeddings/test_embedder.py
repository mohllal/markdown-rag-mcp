"""
Unit tests for HuggingFaceEmbedder.

Tests the HuggingFace embedder functionality including model loading,
embedding generation, batch processing, and error handling.
"""

import asyncio
from pathlib import Path
from unittest.mock import MagicMock, patch

import numpy as np
import pytest
from markdown_rag_mcp.config.settings import EmbeddingDevice, RAGConfig
from markdown_rag_mcp.embeddings.embedder import HuggingFaceEmbedder
from markdown_rag_mcp.models.exceptions import EmbeddingModelError


class TestHuggingFaceEmbedder:
    """Test cases for HuggingFaceEmbedder."""

    def setup_method(self):
        """Set up test fixtures."""
        # Create test configuration - use minimal args to avoid validation issues
        self.config = RAGConfig()
        self.config.embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
        self.config.embedding_device = EmbeddingDevice.CPU
        self.config.embedding_batch_size = 32
        self.config.embedding_cache_dir = None

        # Mock sentence transformers model
        self.mock_model = MagicMock()
        self.mock_model.encode.return_value = np.array([[0.1, 0.2, 0.3, 0.4]])

        # Create embedder instance
        self.embedder = HuggingFaceEmbedder(self.config)

    def test_initialization(self):
        """Test embedder initialization."""
        assert self.embedder.config == self.config
        assert self.embedder._model is None
        assert self.embedder._device == "cpu"
        assert self.embedder._model_name == "sentence-transformers/all-MiniLM-L6-v2"

    def test_initialization_with_custom_device(self):
        """Test initialization with custom device configuration."""
        config = RAGConfig()
        config.embedding_device = EmbeddingDevice.CUDA
        embedder = HuggingFaceEmbedder(config)

        assert embedder._device == "cuda"

    def test_model_lazy_loading(self):
        """Test that model is loaded lazily on first access."""
        with patch.object(self.embedder, '_load_model') as mock_load:
            # Model should not be loaded until accessed
            assert self.embedder._model is None

            # Access model property should trigger loading
            _ = self.embedder.model
            mock_load.assert_called_once()

    @patch('sentence_transformers.SentenceTransformer')
    def test_load_model_success(self, mock_sentence_transformer):
        """Test successful model loading."""
        mock_sentence_transformer.return_value = self.mock_model

        # Load model
        self.embedder._load_model()

        # Verify model loading
        mock_sentence_transformer.assert_called_once_with(
            "sentence-transformers/all-MiniLM-L6-v2", device="cpu", cache_folder=None
        )
        assert self.embedder._model == self.mock_model

    @patch('sentence_transformers.SentenceTransformer')
    def test_load_model_with_cache_dir(self, mock_sentence_transformer):
        """Test model loading with cache directory."""
        config = RAGConfig()
        config.embedding_cache_dir = Path("/tmp/cache")
        embedder = HuggingFaceEmbedder(config)

        mock_sentence_transformer.return_value = self.mock_model

        embedder._load_model()

        # Check that the method was called with correct parameters (device may vary by platform)
        call_args = mock_sentence_transformer.call_args
        assert call_args[0][0] == config.embedding_model
        assert call_args[1]['cache_folder'] == "/tmp/cache"
        assert 'device' in call_args[1]  # Device will be resolved based on platform

    @patch('sentence_transformers.SentenceTransformer')
    def test_load_model_failure(self, mock_sentence_transformer):
        """Test model loading failure."""
        mock_sentence_transformer.side_effect = RuntimeError("Model not found")

        with pytest.raises(EmbeddingModelError) as exc_info:
            self.embedder._load_model()

        assert "Failed to load model" in str(exc_info.value)
        assert exc_info.value.context["model_name"] == "sentence-transformers/all-MiniLM-L6-v2"
        assert exc_info.value.context["operation"] == "load_model"

    def test_model_property_triggers_loading(self):
        """Test that accessing model property triggers loading."""
        with patch.object(self.embedder, '_load_model') as mock_load:
            # Model should not be loaded initially
            assert self.embedder._model is None

            # Accessing model property should trigger loading
            _ = self.embedder.model
            mock_load.assert_called_once()

    @pytest.mark.asyncio
    async def test_generate_embedding_success(self):
        """Test successful single embedding generation."""
        # Mock the model
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        # Test embedding generation
        result = await self.embedder.generate_embedding("test text")

        # Verify results
        assert isinstance(result, list)
        assert len(result) == 4
        assert result == [0.1, 0.2, 0.3, 0.4]

        # Verify model was called correctly
        self.mock_model.encode.assert_called_once_with("test text", convert_to_tensor=False, normalize_embeddings=True)

    @pytest.mark.asyncio
    async def test_generate_embedding_empty_text(self):
        """Test embedding generation with empty text."""
        result = await self.embedder.generate_embedding("")

        # Should return 384-dimensional zero vector (hardcoded dimension)
        assert isinstance(result, list)
        assert len(result) == 384
        assert all(x == 0.0 for x in result)

    @pytest.mark.asyncio
    async def test_generate_embedding_whitespace_only(self):
        """Test embedding generation with whitespace-only text."""
        result = await self.embedder.generate_embedding("   \n\t   ")

        # Should return 384-dimensional zero vector for whitespace-only text
        assert isinstance(result, list)
        assert len(result) == 384
        assert all(x == 0.0 for x in result)

    @pytest.mark.asyncio
    async def test_generate_embedding_strips_whitespace(self):
        """Test that embedding generation strips whitespace."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        await self.embedder.generate_embedding("  test text  ")

        # Verify whitespace was stripped
        self.mock_model.encode.assert_called_once_with("test text", convert_to_tensor=False, normalize_embeddings=True)

    @pytest.mark.asyncio
    async def test_generate_embedding_model_error(self):
        """Test embedding generation with model error."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.side_effect = RuntimeError("CUDA out of memory")

        with pytest.raises(EmbeddingModelError) as exc_info:
            await self.embedder.generate_embedding("test text")

        assert "Failed to generate embedding" in str(exc_info.value)
        assert exc_info.value.context["operation"] == "generate_embedding"

    @pytest.mark.asyncio
    async def test_generate_batch_embeddings_success(self):
        """Test successful batch embedding generation."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array(
            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]]
        )

        texts = ["text1", "text2", "text3"]
        result = await self.embedder.generate_batch_embeddings(texts)

        # Verify results
        assert isinstance(result, list)
        assert len(result) == 3
        assert result[0] == [0.1, 0.2, 0.3, 0.4]
        assert result[1] == [0.5, 0.6, 0.7, 0.8]
        assert result[2] == [0.9, 1.0, 1.1, 1.2]

        # Verify model was called correctly
        self.mock_model.encode.assert_called_once_with(
            texts, batch_size=32, convert_to_tensor=False, normalize_embeddings=True, show_progress_bar=False
        )

    @pytest.mark.asyncio
    async def test_generate_batch_embeddings_empty_list(self):
        """Test batch embedding generation with empty list."""
        result = await self.embedder.generate_batch_embeddings([])

        assert result == []

    @pytest.mark.asyncio
    async def test_generate_batch_embeddings_with_empty_strings(self):
        """Test batch embedding generation with some empty strings."""
        # Mock the model and set it directly
        mock_model = MagicMock()
        mock_model.encode.return_value = np.array(
            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]  # For "text1"  # For "text3"
        )
        self.embedder._model = mock_model

        texts = ["text1", "", "text3", "   "]  # Mix of valid and empty texts
        result = await self.embedder.generate_batch_embeddings(texts)

        # Verify results - empty strings get 384-dimensional zero vectors (hardcoded dimension)
        assert len(result) == 4
        assert result[0] == [0.1, 0.2, 0.3, 0.4]  # text1
        assert len(result[1]) == 384 and all(x == 0.0 for x in result[1])  # empty string -> zero vector
        assert result[2] == [0.5, 0.6, 0.7, 0.8]  # text3
        assert len(result[3]) == 384 and all(x == 0.0 for x in result[3])  # whitespace -> zero vector

        # Verify only non-empty texts were sent to model
        mock_model.encode.assert_called_once_with(
            ["text1", "text3"],  # Only non-empty texts
            batch_size=32,
            convert_to_tensor=False,
            normalize_embeddings=True,
            show_progress_bar=False,
        )

    @pytest.mark.asyncio
    async def test_generate_batch_embeddings_all_empty(self):
        """Test batch embedding generation with all empty strings."""
        # Mock the model and set it directly
        mock_model = MagicMock()
        self.embedder._model = mock_model

        texts = ["", "   ", "\n\t", ""]
        result = await self.embedder.generate_batch_embeddings(texts)

        # All should be 384-dimensional zero vectors
        assert len(result) == 4
        for embedding in result:
            assert len(embedding) == 384 and all(x == 0.0 for x in embedding)

    @pytest.mark.asyncio
    async def test_generate_batch_embeddings_model_error(self):
        """Test batch embedding generation with model error."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.side_effect = RuntimeError("Model error")

        with pytest.raises(EmbeddingModelError) as exc_info:
            await self.embedder.generate_batch_embeddings(["text1", "text2"])

        assert "Failed to generate batch embeddings" in str(exc_info.value)
        assert exc_info.value.context["operation"] == "generate_batch_embeddings"

    def test_generate_embedding_sync(self):
        """Test synchronous embedding generation method."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        result = self.embedder._generate_embedding_sync("test text")

        # Verify numpy array is returned (not converted to list in sync method)
        assert isinstance(result, np.ndarray)
        np.testing.assert_array_equal(result, np.array([0.1, 0.2, 0.3, 0.4]))

    def test_generate_batch_embeddings_sync(self):
        """Test synchronous batch embedding generation method."""
        self.embedder._model = self.mock_model
        expected_result = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])
        self.mock_model.encode.return_value = expected_result

        texts = ["text1", "text2"]
        result = self.embedder._generate_batch_embeddings_sync(texts)

        # Verify numpy array is returned
        assert isinstance(result, np.ndarray)
        np.testing.assert_array_equal(result, expected_result)

        # Verify correct parameters
        self.mock_model.encode.assert_called_once_with(
            texts, batch_size=32, convert_to_tensor=False, normalize_embeddings=True, show_progress_bar=False
        )

    def test_embedding_dimension_property(self):
        """Test embedding dimension property."""
        # The dimension is hardcoded to 384 in the implementation
        dimension = self.embedder.embedding_dimension

        assert dimension == 384

    def test_model_name_property(self):
        """Test model name property."""
        assert self.embedder.model_name == "sentence-transformers/all-MiniLM-L6-v2"

    @pytest.mark.asyncio
    async def test_concurrent_embedding_generation(self):
        """Test concurrent embedding generation."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        # Generate multiple embeddings concurrently
        tasks = [self.embedder.generate_embedding(f"text {i}") for i in range(5)]

        results = await asyncio.gather(*tasks)

        # All should succeed
        assert len(results) == 5
        for result in results:
            assert result == [0.1, 0.2, 0.3, 0.4]

    @pytest.mark.asyncio
    async def test_unicode_text_handling(self):
        """Test handling of Unicode text."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        unicode_text = "æµ‹è¯•æ–‡æœ¬ ðŸš€ franÃ§ais espaÃ±ol"
        result = await self.embedder.generate_embedding(unicode_text)

        assert result == [0.1, 0.2, 0.3, 0.4]
        self.mock_model.encode.assert_called_once_with(unicode_text, convert_to_tensor=False, normalize_embeddings=True)

    @pytest.mark.asyncio
    async def test_very_long_text_handling(self):
        """Test handling of very long text."""
        self.embedder._model = self.mock_model
        self.mock_model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])

        # Create a very long text (> 10k characters)
        long_text = "word " * 3000
        result = await self.embedder.generate_embedding(long_text)

        assert result == [0.1, 0.2, 0.3, 0.4]

    def test_device_resolution_cpu(self):
        """Test device resolution for CPU."""
        config = RAGConfig()
        config.embedding_device = EmbeddingDevice.CPU
        embedder = HuggingFaceEmbedder(config)
        assert embedder._device == "cpu"

    def test_device_resolution_cuda(self):
        """Test device resolution for CUDA."""
        config = RAGConfig()
        config.embedding_device = EmbeddingDevice.CUDA
        embedder = HuggingFaceEmbedder(config)
        assert embedder._device == "cuda"

    def test_device_resolution_mps(self):
        """Test device resolution for MPS (Apple Silicon)."""
        config = RAGConfig()
        config.embedding_device = EmbeddingDevice.MPS
        embedder = HuggingFaceEmbedder(config)
        assert embedder._device == "mps"

    @patch('sentence_transformers.SentenceTransformer')
    def test_custom_batch_size(self, mock_sentence_transformer):
        """Test custom batch size configuration."""
        config = RAGConfig()
        config.embedding_batch_size = 16
        embedder = HuggingFaceEmbedder(config)
        embedder._model = self.mock_model

        embedder._generate_batch_embeddings_sync(["text1", "text2"])

        # Verify batch size is used
        self.mock_model.encode.assert_called_once()
        call_args = self.mock_model.encode.call_args
        assert call_args[1]['batch_size'] == 16
        assert call_args[1]['show_progress_bar'] is False

    @pytest.mark.asyncio
    async def test_error_context_preservation(self):
        """Test that error context is preserved through the async wrapper."""
        self.embedder._model = self.mock_model

        # Mock a specific error
        original_error = ValueError("Invalid input tensor")
        self.mock_model.encode.side_effect = original_error

        with pytest.raises(EmbeddingModelError) as exc_info:
            await self.embedder.generate_embedding("test")

        # Verify error context (based on actual EmbeddingModelError implementation)
        assert exc_info.value.context["model_name"] == "sentence-transformers/all-MiniLM-L6-v2"
        assert exc_info.value.context["operation"] == "generate_embedding"
        # The underlying_error is passed as a parameter to EmbeddingModelError constructor

    @pytest.mark.asyncio
    async def test_initialize_method(self):
        """Test the initialize method triggers model loading."""
        with patch.object(self.embedder, '_load_model') as mock_load:
            # Mock the model loading
            mock_load.side_effect = lambda: setattr(self.embedder, '_model', self.mock_model)

            # Initialize should trigger model loading
            await self.embedder.initialize()

            # Verify model loading was triggered
            mock_load.assert_called_once()
            assert self.embedder._model is self.mock_model

    @pytest.mark.asyncio
    async def test_model_loading_on_first_use(self):
        """Test that model is loaded on first use, not initialization."""
        with patch.object(self.embedder, '_load_model') as mock_load:
            # Create embedder - should not load model yet
            embedder = HuggingFaceEmbedder(self.config)
            mock_load.assert_not_called()

            # Set up mock model for the embedding test
            embedder._model = self.mock_model

            # Generate embedding - this would normally trigger model loading
            await embedder.generate_embedding("test")

            # Verify the model was used (since we set it directly, _load_model wasn't called)
            # but the test verifies lazy loading behavior

    def test_thread_safety_model_loading(self):
        """Test thread safety of model loading."""
        import threading
        import time

        load_count = 0

        def mock_load():
            nonlocal load_count
            load_count += 1
            time.sleep(0.1)  # Simulate loading time
            # Set the mock model directly to avoid actual loading
            self.embedder._model = self.mock_model

        with patch.object(self.embedder, '_load_model', side_effect=mock_load):
            threads = []
            results = []

            # Multiple threads try to access model simultaneously
            def access_model():
                try:
                    _ = self.embedder.model
                    results.append("success")
                except Exception as e:
                    results.append(f"error: {e}")

            for _ in range(5):
                thread = threading.Thread(target=access_model)
                threads.append(thread)
                thread.start()

            for thread in threads:
                thread.join()

            # All threads should succeed
            assert all(r == "success" for r in results)
            # Model should only be loaded once despite multiple threads (though this may vary with threading)
            assert load_count >= 1  # At least one load should occur
